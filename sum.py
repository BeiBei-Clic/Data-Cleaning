def generate_summary(self, text: str, filename: str) -> str:
    """ç”Ÿæˆæ‘˜è¦ï¼ˆé‡‡ç”¨åˆ†å—æ¸…æ´—ç›¸åŒçš„åˆ†å—é€»è¾‘ï¼‰"""
    CHUNK_SIZE = 2000 # æ–‡ä»¶å¤§å°è¶…è¿‡2000å­—ç¬¦æ—¶è¿›è¡Œåˆ†å—
    OVERLAP_SIZE = self.overlap_size

    prompt_template = """è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯ä½œä¸ºéƒ¨åˆ†æ‘˜è¦ï¼š

å½“å‰æ–‡æ¡£: {filename}
åˆ†å—: {chunk_num}/{total_chunks}

è¦æ±‚ï¼š
1. æå–æœ¬éƒ¨åˆ†çš„æ ¸å¿ƒå†…å®¹
2. ä¿æŒå®¢è§‚äº‹å®
3. ç”¨ç®€æ´çš„çŸ­è¯­åˆ—å‡ºè¦ç‚¹

æ–‡æœ¬å†…å®¹ï¼š
{chunk_text}"""

    final_prompt = """è¯·å°†ä»¥ä¸‹éƒ¨åˆ†æ‘˜è¦åˆå¹¶ä¸ºå®Œæ•´æ‘˜è¦ï¼ŒæŒ‰ç»“æ„æ•´ç†ï¼š

ã€æ ‡é¢˜ã€‘<ä¿æŒåŸæ–‡æ ‡é¢˜ä¸å˜>

- é¡¹ç›®èƒŒæ™¯ï¼ˆæ•´åˆå„éƒ¨åˆ†èƒŒæ™¯ï¼‰
- ä¸»è¦æªæ–½ï¼ˆåˆå¹¶æ‰€æœ‰æªæ–½ï¼Œå»é‡åä¿ç•™3-5ç‚¹ï¼‰
- å–å¾—æˆæ•ˆï¼ˆåˆå¹¶é‡åŒ–æˆæœï¼‰
- ç»éªŒæ•™è®­ï¼ˆæ•´åˆå…³é”®ç»éªŒï¼‰

è¦æ±‚ï¼šæœ€ç»ˆæ‘˜è¦ä¸è¶…è¿‡600å­—

éƒ¨åˆ†æ‘˜è¦ï¼š
{partial_summaries}"""

    # åˆ†å—é€»è¾‘ï¼ˆä¸clean_text_chunksä¿æŒä¸€è‡´ï¼‰
    def split_into_chunks(text: str) -> list:
        if len(text) <= CHUNK_SIZE:
            return [text]

        chunks = []
        start = 0
        while start < len(text):
            end = start + CHUNK_SIZE
            if end < len(text):
                # ä¼˜å…ˆåœ¨æ®µè½è¾¹ç•Œåˆ†å‰²
                for separator in ['\n\n', 'ã€‚', '\n', ' ']:
                    search_start = max(start + CHUNK_SIZE - OVERLAP_SIZE, start)
                    split_pos = text.rfind(separator, search_start, end)
                    if split_pos != -1:
                        end = split_pos + len(separator)
                        break
            chunks.append(text[start:end].strip())
            start = max(end - OVERLAP_SIZE, start + 1)
        return chunks

    # å¤„ç†å•ä¸ªåˆ†å—
    def process_chunk(chunk: str, chunk_num: int, total_chunks: int) -> str:
        prompt = prompt_template.format(
            filename=filename,
            chunk_num=chunk_num + 1,
            total_chunks=total_chunks,
            chunk_text=chunk
        )

        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3,
                    max_tokens=300  # é™åˆ¶éƒ¨åˆ†æ‘˜è¦é•¿åº¦
                )
                if response and response.choices:
                    return response.choices[0].message.content
            except Exception as e:
                print(f"åˆ†å—{chunk_num + 1}ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥: {str(e)}")
                if attempt < self.max_retries - 1:
                    time.sleep(30)
        return f"[åˆ†å—{chunk_num + 1}æ‘˜è¦ç”Ÿæˆå¤±è´¥]"

    # ä¸»æµç¨‹
    chunks = split_into_chunks(text)
    if len(chunks) == 1:
        # å°æ–‡ä»¶ç›´æ¥å¤„ç†ï¼ˆä½¿ç”¨åŸå§‹æç¤ºæ¨¡æ¿ï¼‰
        prompt = f"""è¯·ä»ä»¥ä¸‹æ¡ˆä¾‹ä¸­æå–å…³é”®ä¿¡æ¯ï¼ŒæŒ‰ä»¥ä¸‹ç»“æ„æ€»ç»“ï¼š

ã€æ ‡é¢˜ã€‘<ä¿æŒåŸæ–‡æ ‡é¢˜ä¸å˜>

- é¡¹ç›®èƒŒæ™¯
- ä¸»è¦æªæ–½ï¼ˆ3-5ç‚¹ï¼‰
- å–å¾—æˆæ•ˆï¼ˆ2-3é¡¹å…·ä½“æˆæœï¼‰
- ç»éªŒæ•™è®­ï¼ˆ3-5ç‚¹ï¼‰

è¦æ±‚ï¼šå­—æ•°ä¸è¶…è¿‡600å­—

æ¡ˆä¾‹[{filename}]ï¼š
{text}"""
        return self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        ).choices[0].message.content

    # å¤§æ–‡ä»¶åˆ†å—å¤„ç†
    print(f"ğŸ“‘ å¤§æ–‡ä»¶åˆ†å—å¤„ç†: {filename} (å…±{len(chunks)}å—)")

    partial_summaries = []
    for i, chunk in enumerate(chunks):
        print(f"  æ­£åœ¨å¤„ç†åˆ†å— {i + 1}/{len(chunks)}...")
        chunk_summary = process_chunk(chunk, i, len(chunks))
        partial_summaries.append(f"=== åˆ†å— {i + 1} ===\n{chunk_summary}")

    # åˆå¹¶æ‘˜è¦
    print("ğŸ”„ åˆå¹¶éƒ¨åˆ†æ‘˜è¦...")
    combined_text = "\n\n".join(partial_summaries)
    final_summary = self.client.chat.completions.create(
        model=self.model,
        messages=[{"role": "user", "content": final_prompt.format(partial_summaries=combined_text)}],
        temperature=0.3,
        max_tokens=600
    ).choices[0].message.content

    return final_summary