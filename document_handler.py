import os
import time
import json
import warnings
from pathlib import Path
from typing import Dict

import requests
import docx
import PyPDF2
from dotenv import load_dotenv
from openai import OpenAI

class DocumentHandler:
    def __init__(self):
        """
        ========================================
        æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ– - æ–°æ‰‹ä½¿ç”¨æŒ‡å—
        ========================================
        
        ğŸš€ å¿«é€Ÿå¼€å§‹æ­¥éª¤ï¼š
        1. åˆ›å»º .env æ–‡ä»¶å¹¶é…ç½®å¿…è¦å‚æ•°ï¼ˆè§ä¸‹æ–¹è¯¦ç»†è¯´æ˜ï¼‰
        2. åˆ›å»º input_files æ–‡ä»¶å¤¹ï¼Œæ”¾å…¥è¦å¤„ç†çš„æ–‡æ¡£
        3. è¿è¡Œç¨‹åºï¼špython document_handler.py
        
        ğŸ“‹ å¿…éœ€çš„ .env æ–‡ä»¶é…ç½®ï¼š
        åˆ›å»ºé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶ï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
        
        # å¤§æ¨¡å‹APIé…ç½®ï¼ˆç”¨äºç”Ÿæˆæ‘˜è¦å’Œæ¸…æ´—æ–‡æœ¬ä¸ä¸€å®šæ˜¯OPENROUTERï¼Œè®°å¾—åœ¨inité‚£é‡Œæ”¹æˆä½ å¯¹åº”çš„å°±è¡Œï¼‰
        OPENROUTER_API_KEY=your_openrouter_api_key_here
        OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
        
        # DifyçŸ¥è¯†åº“é…ç½®ï¼ˆç”¨äºå­˜å‚¨å¤„ç†åçš„æ–‡æ¡£ï¼‰
        DIFY_API_KEY=your_dify_api_key_here
        DIFY_BASE_URL=http://your_dify_server_url/v1
        SUMMARY_DATASET_ID=your_summary_dataset_id_hereï¼ˆå­˜æ”¾æ‘˜è¦çš„çŸ¥è¯†åº“ï¼‰
        ORIGINAL_DATASET_ID=your_original_dataset_id_hereï¼ˆå­˜æ”¾åŸæ–‡çš„çŸ¥è¯†åº“ï¼‰
        
        ğŸ“ æ–‡ä»¶å¤¹è¯´æ˜ï¼š
        - input_files/          : ã€å¿…é¡»åˆ›å»ºã€‘æ”¾å…¥è¦å¤„ç†çš„æ–‡æ¡£ï¼ˆPDFã€Wordã€MDã€TXTï¼‰
        - temp_summaries/       : ã€è‡ªåŠ¨åˆ›å»ºã€‘ä¸´æ—¶å­˜å‚¨ç”Ÿæˆçš„æ‘˜è¦æ–‡ä»¶
        - temp_cleaned_originals/ : ã€è‡ªåŠ¨åˆ›å»ºã€‘ä¸´æ—¶å­˜å‚¨æ¸…æ´—åçš„åŸæ–‡
        - temp_cleaned_summaries/ : ã€è‡ªåŠ¨åˆ›å»ºã€‘ä¸´æ—¶å­˜å‚¨æ¸…æ´—åçš„æ‘˜è¦
        
        âš ï¸ é‡è¦æé†’ï¼š
        å¦‚æœ temp_ å¼€å¤´çš„æ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œè¯·å…ˆæ‰‹åŠ¨åˆ é™¤ï¼
        è¿™äº›æ˜¯ä¸­é—´å¤„ç†æ–‡ä»¶ï¼Œå¯èƒ½åŒ…å«ä¸Šæ¬¡æœªå®Œæˆçš„æ•°æ®ã€‚
        
        ğŸ”§ å¯è°ƒå‚æ•°è¯´æ˜ï¼š
        ä¸‹æ–¹å‚æ•°å¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œæ— éœ€ä¿®æ”¹ä»£ç å…¶ä»–éƒ¨åˆ†
        """
        
        # åŠ è½½ç¯å¢ƒå˜é‡é…ç½®
        load_dotenv()
        
        # ==================== å¤§æ¨¡å‹é…ç½® ====================
        # ç”¨äºç”Ÿæˆæ‘˜è¦å’Œæ¸…æ´—æ–‡æœ¬çš„AIæ¨¡å‹é…ç½®
        self.client = OpenAI(
            api_key=os.getenv('OPENROUTER_API_KEY'),    # ä».envæ–‡ä»¶è¯»å–APIå¯†é’¥
            base_url=os.getenv('OPENROUTER_BASE_URL')   # ä».envæ–‡ä»¶è¯»å–APIåœ°å€
        )
        
        # ğŸ”§ å¯è°ƒå‚æ•°ï¼šé€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹
        # æ¨èæ¨¡å‹ï¼š'google/gemini-2.5-flash' (å¿«é€Ÿä¾¿å®œ)
        # å…¶ä»–é€‰æ‹©ï¼š'anthropic/claude-3-haiku', 'openai/gpt-4o-mini'
        self.model = 'google/gemini-2.5-flash'
        
        # ==================== DifyçŸ¥è¯†åº“é…ç½® ====================
        # Difyæ˜¯ç”¨äºå­˜å‚¨å’Œç®¡ç†å¤„ç†åæ–‡æ¡£çš„çŸ¥è¯†åº“ç³»ç»Ÿ
        self.dify_api_key = os.getenv('DIFY_API_KEY')                    # Dify APIå¯†é’¥
        self.dify_base_url = os.getenv('DIFY_BASE_URL', 'http://localhost/v1')  # DifyæœåŠ¡åœ°å€
        self.summary_dataset_id = os.getenv('SUMMARY_DATASET_ID')       # æ‘˜è¦çŸ¥è¯†åº“ID
        self.original_dataset_id = os.getenv('ORIGINAL_DATASET_ID')     # åŸæ–‡çŸ¥è¯†åº“ID
        
        # ==================== æ–‡æœ¬å¤„ç†å‚æ•° ====================
        # ğŸ”§ å¯è°ƒå‚æ•°ï¼šæ–‡æœ¬åˆ†å—å¤„ç†è®¾ç½®
        self.chunk_size = 3000      # æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆå»ºè®®2000-5000ï¼‰
        self.overlap_size = 500     # æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼ˆå»ºè®®chunk_sizeçš„10-20%ï¼‰
        self.max_retries = 3        # APIè°ƒç”¨å¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°
        
        # ==================== DifyçŸ¥è¯†åº“åˆ†å—å‚æ•° ====================
        # è¿™äº›å‚æ•°æ§åˆ¶æ–‡æ¡£åœ¨Difyä¸­çš„å­˜å‚¨å’Œæ£€ç´¢æ–¹å¼
        # ğŸ”§ å¯è°ƒå‚æ•°ï¼šæ ¹æ®æ–‡æ¡£ç±»å‹å’Œæ£€ç´¢éœ€æ±‚è°ƒæ•´
        self.parent_mode = "paragraph"          # çˆ¶çº§åˆ†å—æ¨¡å¼ï¼šparagraph(æ®µè½) æˆ– sentence(å¥å­)
        self.parent_separator = "&&&&"          # çˆ¶çº§åˆ†å—åˆ†éš”ç¬¦ï¼ˆä¸è¦ä¿®æ”¹ï¼Œé™¤éäº†è§£Difyæœºåˆ¶ï¼‰
        self.parent_max_tokens = 4000           # çˆ¶çº§åˆ†å—æœ€å¤§tokenæ•°ï¼ˆå»ºè®®3000-6000ï¼‰
        self.subchunk_separator = "###"         # å­åˆ†å—åˆ†éš”ç¬¦ï¼ˆç”¨äºå…³é”®è¯æ ‡è®°ï¼‰
        self.subchunk_max_tokens = 96           # å­åˆ†å—æœ€å¤§tokenæ•°ï¼ˆå»ºè®®64-128ï¼‰
        
        # ==================== ä¸´æ—¶æ–‡ä»¶å¤¹é…ç½® ====================
        # ç¨‹åºè¿è¡Œè¿‡ç¨‹ä¸­çš„ä¸­é—´æ–‡ä»¶å­˜å‚¨ä½ç½®
        self.summary_dir = "temp_summaries"              # å­˜å‚¨ç”Ÿæˆçš„æ‘˜è¦
        self.cleaned_original_dir = "temp_cleaned_originals"  # å­˜å‚¨æ¸…æ´—åçš„åŸæ–‡
        self.cleaned_summary_dir = "temp_cleaned_summaries"   # å­˜å‚¨æ¸…æ´—åçš„æ‘˜è¦
        
        # è‡ªåŠ¨åˆ›å»ºå¿…è¦çš„ç›®å½•
        # input_files: ç”¨æˆ·æ”¾å…¥åŸå§‹æ–‡æ¡£çš„æ–‡ä»¶å¤¹
        # temp_*: ç¨‹åºå¤„ç†è¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ–‡ä»¶å¤¹
        for directory in ["input_files", self.summary_dir, self.cleaned_original_dir, self.cleaned_summary_dir]:
            os.makedirs(directory, exist_ok=True)
        
        # ==================== å†…å­˜æ•°æ®å­˜å‚¨ ====================
        # ç¨‹åºè¿è¡Œæ—¶åœ¨å†…å­˜ä¸­ä¸´æ—¶å­˜å‚¨å¤„ç†ç»“æœ
        self.summaries: Dict[str, str] = {}         # å­˜å‚¨ç”Ÿæˆçš„æ‘˜è¦ {æ–‡ä»¶å: æ‘˜è¦å†…å®¹}
        self.cleaned_originals: Dict[str, str] = {} # å­˜å‚¨æ¸…æ´—åçš„åŸæ–‡ {æ–‡ä»¶å: æ¸…æ´—åå†…å®¹}
        self.cleaned_summaries: Dict[str, str] = {} # å­˜å‚¨æ¸…æ´—åçš„æ‘˜è¦ {æ–‡ä»¶å: æ¸…æ´—åæ‘˜è¦}
        
        print("âœ… æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼")
        print("ğŸ“ è¯·å°†è¦å¤„ç†çš„æ–‡æ¡£æ”¾å…¥ input_files/ æ–‡ä»¶å¤¹")
        print("ğŸ“„ æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼šPDFã€Word(.docx)ã€Markdown(.md)ã€æ–‡æœ¬(.txt)")

    def read_file(self, file_path: str) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        path = Path(file_path)
        ext = path.suffix.lower()
        
        if ext == '.docx':
            doc = docx.Document(path)
            return '\n'.join([p.text for p in doc.paragraphs])
        elif ext == '.pdf':
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                with open(path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    return '\n'.join([page.extract_text() for page in reader.pages])
        else:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()

    def generate_summary(self, text: str, filename: str) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        prompt = f"""è¯·ä»ä»¥ä¸‹æ¡ˆä¾‹ä¸­æå–å…³é”®ä¿¡æ¯ï¼ŒæŒ‰ä»¥ä¸‹ç»“æ„æ€»ç»“ï¼š

ã€æ ‡é¢˜ã€‘<ä¿æŒåŸæ–‡æ ‡é¢˜ä¸å˜>

- é¡¹ç›®èƒŒæ™¯
- ä¸»è¦æªæ–½ï¼ˆ3-5ç‚¹ï¼‰
- å–å¾—æˆæ•ˆï¼ˆ2-3é¡¹å…·ä½“æˆæœï¼‰
- ç»éªŒæ•™è®­ï¼ˆ3-5ç‚¹ï¼‰

è¦æ±‚ï¼šå­—æ•°ä¸è¶…è¿‡600å­—

æ¡ˆä¾‹[{filename}]ï¼š
{text}"""

        for attempt in range(self.max_retries):
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            
            if response and response.choices and response.choices[0].message.content:
                return response.choices[0].message.content
            
            if attempt < self.max_retries - 1:
                time.sleep(30)
        
        return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {filename}"

    def clean_text_chunks(self, text: str) -> str:
        """åˆ†å—æ¸…æ´—æ–‡æœ¬"""
        # åˆ†å‰²æ–‡æœ¬
        if len(text) <= self.chunk_size:
            chunks = [text]
        else:
            chunks = []
            start = 0
            
            while start < len(text):
                end = start + self.chunk_size
                
                if end < len(text):
                    for char in ['\n\n', 'ã€‚', '\n', ' ']:
                        search_start = max(start + self.chunk_size - self.overlap_size, start)
                        char_pos = text.rfind(char, search_start, end)
                        if char_pos != -1:
                            end = char_pos + len(char)
                            break
                
                chunk = text[start:end].strip()
                if chunk:
                    chunks.append(chunk)
                
                if end >= len(text):
                    break
                
                start = max(end - self.overlap_size, start + 1)
        
        # å¤„ç†æ–‡æœ¬å—
        results = []
        for i, chunk in enumerate(chunks):
            print(f"æ­£åœ¨å¤„ç†æ–‡æœ¬å— {i+1}/{len(chunks)}...")
            result = self.clean_single_chunk(i, chunk)
            results.append(result)
        
        # ç”¨&&&&è¿æ¥å„ä¸ªæ–‡æœ¬å—
        return f"\n{self.parent_separator}\n".join(results)

    def clean_single_chunk(self, index: int, chunk_text: str) -> str:
        """æ¸…æ´—å•ä¸ªæ–‡æœ¬å—"""
        prompt = f"""è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ¸…æ´—å’Œå…³é”®è¯æå–ï¼š

æ¸…æ´—è¦æ±‚ï¼š
1. ä¿æŒåŸè¯­è¨€ï¼Œåˆ é™¤é¡µçœ‰é¡µè„šã€HTMLæ ‡ç­¾ã€å¤šä½™ç©ºç™½
2. ä¿®æ­£æ®µè½åˆ†å‰²ï¼Œæ¸…ç†ä¹±ç 
3. ä¿ç•™é‡è¦ä¿¡æ¯ã€æ•°æ®ã€åœ°åç­‰

å…³é”®è¯æå–è¦æ±‚ï¼š
- æå–8-12ä¸ªç›¸å…³å…³é”®è¯
- é‡ç‚¹å…³æ³¨ï¼šç»è¥æ¨¡å¼ã€äº§ä¸šç±»å‹ã€æŠ€æœ¯åº”ç”¨ã€æ”¿ç­–æªæ–½

è¾“å‡ºæ ¼å¼ï¼š
æ¸…æ´—åæ–‡æœ¬ï¼š
[æ¸…æ´—åçš„æ–‡æœ¬å†…å®¹]
å…³é”®è¯ï¼š
[å…³é”®è¯1, å…³é”®è¯2, å…³é”®è¯3, ...]

åŸæ–‡æœ¬ï¼š
{chunk_text}
"""
        
        for attempt in range(self.max_retries):
            print(f"  å°è¯•ç¬¬ {attempt + 1} æ¬¡è°ƒç”¨API...")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            
            if response and response.choices and response.choices[0].message.content:
                content = response.choices[0].message.content
                
                # è§£ææ¸…æ´—åçš„æ–‡æœ¬å’Œå…³é”®è¯
                if "æ¸…æ´—åæ–‡æœ¬ï¼š" in content:
                    parts = content.split("å…³é”®è¯ï¼š")
                    cleaned_text = parts[0].replace("æ¸…æ´—åæ–‡æœ¬ï¼š", "").strip()
                    keywords = parts[1].split("åŸæ–‡æœ¬ï¼š")[0].strip()
                    
                    # æ ¼å¼åŒ–å…³é”®è¯
                    keyword_list = [kw.strip() for kw in keywords.split(',') if kw.strip()]
                    formatted_keywords = "".join([f"{self.subchunk_separator}{kw}" for kw in keyword_list])
                    
                    print(f"  âœ… æ–‡æœ¬å— {index + 1} å¤„ç†æˆåŠŸ")
                    # è¿”å›æ ¼å¼ï¼šæ­£æ–‡ + æ¢è¡Œ + å…³é”®è¯
                    return f"{cleaned_text}\n{formatted_keywords}"
                
                # å¦‚æœæ²¡æœ‰æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è¿”å›å†…å®¹
                print(f"  âš ï¸ æ ¼å¼è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è¿”å›å†…å®¹")
                return f"{content}\n{self.subchunk_separator}å¤„ç†å¤±è´¥"
            
            if attempt < self.max_retries - 1:
                print(f"  âš ï¸ APIè°ƒç”¨å¤±è´¥ï¼Œç­‰å¾…30ç§’åé‡è¯•...")
                time.sleep(30)
        
        # å¤±è´¥æ—¶è¿”å›åŸæ–‡æœ¬
        return f"{self.parent_separator}\n{chunk_text.strip()}"

    def upload_to_dify(self, data_dict: Dict[str, str], dataset_id: str, data_type: str) -> bool:
        """ä¸Šä¼ åˆ°DifyçŸ¥è¯†åº“"""
        max_case_id = max(
            self.get_max_case_id(self.summary_dataset_id),
            self.get_max_case_id(self.original_dataset_id)
        )
        
        upload_results = []
        sorted_files = sorted(data_dict.items())
        
        for i, (filename, content) in enumerate(sorted_files):
            case_id = max_case_id + i + 1
            temp_filename = f"{Path(filename).stem}_cleaned_{data_type}.md"
            
            # æ ¹æ®æ•°æ®ç±»å‹ä¿å­˜åˆ°ä¸åŒç›®å½•
            temp_dir = "temp_cleaned_originals" if data_type == "original" else "temp_cleaned_summaries"
            os.makedirs(temp_dir, exist_ok=True)
            temp_path = os.path.join(temp_dir, temp_filename)
            
            # å†™ä¸´æ—¶æ–‡ä»¶
            with open(temp_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            # ä¸Šä¼ æ–‡ä»¶
            success = self.upload_file(temp_path, temp_filename, case_id, dataset_id)
            upload_results.append(success)
            os.remove(temp_path)
                        
            print(f"{'âœ…' if success else 'âŒ'} ä¸Šä¼ : {filename} (case_id: {case_id})")
        
        return all(upload_results)

    def get_max_case_id(self, dataset_id: str) -> int:
        """è·å–æœ€å¤§case_id"""
        url = f"{self.dify_base_url}/datasets/{dataset_id}/documents"
        headers = {'Authorization': f'Bearer {self.dify_api_key}'}
        
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            return 0
            
        documents = response.json().get('data', [])
        max_case_id = 0
        
        for doc in documents:
            for meta in doc.get('doc_metadata', []):
                if meta.get('name') == 'case_id':
                    case_id = meta.get('value')
                    if case_id and str(case_id).isdigit():
                        max_case_id = max(max_case_id, int(case_id))
        
        return max_case_id

    def upload_file(self, file_path: str, filename: str, case_id: int, dataset_id: str) -> bool:
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶"""
        url = f"{self.dify_base_url}/datasets/{dataset_id}/document/create-by-file"
        headers = {'Authorization': f'Bearer {self.dify_api_key}'}
        
        data_payload = {
            "indexing_technique": "high_quality",
            "doc_form": "hierarchical_model",
            "process_rule": {
                "mode": "hierarchical",
                "rules": {
                    "pre_processing_rules": [
                        {"id": "remove_extra_spaces", "enabled": True},
                        {"id": "remove_urls_emails", "enabled": True}
                    ],
                    "parent_mode": self.parent_mode,
                    "segmentation": {
                        "separator": self.parent_separator,
                        "max_tokens": self.parent_max_tokens
                    },
                    "subchunk_segmentation": {
                        "separator": self.subchunk_separator,
                        "max_tokens": self.subchunk_max_tokens
                    }
                }
            }
        }
        
        with open(file_path, 'rb') as f:
            files = {'file': (filename, f)}
            data_str = json.dumps(data_payload, ensure_ascii=False)
            response = requests.post(url, headers=headers, files=files, data={'data': data_str})
        
        if response.status_code == 200:
            document_id = response.json()['document']['id']
            return self.update_metadata(document_id, case_id, dataset_id)
        
        return False

    def update_metadata(self, document_id: str, case_id: int, dataset_id: str) -> bool:
        """æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®"""
        # è·å–å…ƒæ•°æ®å­—æ®µID
        metadata_url = f"{self.dify_base_url}/datasets/{dataset_id}/metadata"
        headers = {'Authorization': f'Bearer {self.dify_api_key}'}
        response = requests.get(metadata_url, headers=headers)
        
        if response.status_code != 200:
            return False
        
        case_id_field_id = None
        for field in response.json().get('doc_metadata', []):
            if field.get('name') == 'case_id':
                case_id_field_id = field['id']
                break
        
        if not case_id_field_id:
            return False
        
        # æ›´æ–°å…ƒæ•°æ®
        url = f"{self.dify_base_url}/datasets/{dataset_id}/documents/metadata"
        headers = {
            'Authorization': f'Bearer {self.dify_api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            "operation_data": [{
                "document_id": document_id,
                "metadata_list": [{
                    "id": case_id_field_id,
                    "value": str(case_id),
                    "name": "case_id"
                }]
            }]
        }
        
        response = requests.post(url, headers=headers, json=data)
        return response.status_code == 200

    def upload_paired_documents(self) -> bool:
        """åŒæ—¶ä¸Šä¼ æ‘˜è¦å’ŒåŸæ–‡ï¼Œç¡®ä¿case_idä¸€è‡´"""
        if not self.cleaned_summaries:
            print("æ²¡æœ‰æ¸…æ´—åçš„æ‘˜è¦éœ€è¦ä¸Šä¼ ")
            return True
            
        # è·å–æœ€å¤§case_id
        max_case_id = max(
            self.get_max_case_id(self.summary_dataset_id),
            self.get_max_case_id(self.original_dataset_id)
        )
        
        upload_results = []
        sorted_summaries = sorted(self.cleaned_summaries.items())
        
        for i, (filename, summary_content) in enumerate(sorted_summaries):
            case_id = max_case_id + i + 1
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„åŸæ–‡
            original_content = self.cleaned_originals.get(filename)
            if not original_content:
                print(f"âš ï¸ æ–‡ä»¶ {filename} æ²¡æœ‰å¯¹åº”çš„æ¸…æ´—ååŸæ–‡ï¼Œè·³è¿‡")
                continue
            
            print(f"æ­£åœ¨ä¸Šä¼ æ–‡ä»¶å¯¹: {filename} (case_id: {case_id})")
            
            # ä¸Šä¼ æ‘˜è¦
            summary_filename = f"{Path(filename).stem}_cleaned_summary.md"
            summary_temp_path = os.path.join("temp_cleaned_summaries", summary_filename)
            os.makedirs("temp_cleaned_summaries", exist_ok=True)
            
            with open(summary_temp_path, 'w', encoding='utf-8') as f:
                f.write(summary_content)
            
            summary_success = self.upload_file(summary_temp_path, summary_filename, case_id, self.summary_dataset_id)
            os.remove(summary_temp_path)
            
            if not summary_success:
                print(f"âŒ æ‘˜è¦ä¸Šä¼ å¤±è´¥: {filename}")
                upload_results.append(False)
                continue
            
            # ä¸Šä¼ åŸæ–‡
            original_filename = f"{Path(filename).stem}_cleaned_original.md"
            original_temp_path = os.path.join("temp_cleaned_originals", original_filename)
            os.makedirs("temp_cleaned_originals", exist_ok=True)
            
            with open(original_temp_path, 'w', encoding='utf-8') as f:
                f.write(original_content)
            
            original_success = self.upload_file(original_temp_path, original_filename, case_id, self.original_dataset_id)
            os.remove(original_temp_path)
            
            if summary_success and original_success:
                print(f"âœ… æ–‡ä»¶å¯¹ä¸Šä¼ æˆåŠŸ: {filename} (case_id: {case_id})")
                upload_results.append(True)
            else:
                print(f"âŒ åŸæ–‡ä¸Šä¼ å¤±è´¥: {filename}")
                upload_results.append(False)
        
        return all(upload_results)

    def process_documents(self, input_dir: str):
        """ä¸»å¤„ç†æµç¨‹"""
        
        files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.pdf', '.docx', '.md', '.txt'))]
        
        if files:
            print(f"å‘ç° {len(files)} ä¸ªæ–°æ–‡ä»¶éœ€è¦å¤„ç†...")
            
            # æ­¥éª¤1: ç”Ÿæˆæ‘˜è¦
            print("=== æ­¥éª¤1: ç”Ÿæˆæ‘˜è¦ ===")
            for filename in files:
                filepath = os.path.join(input_dir, filename)
                text = self.read_file(filepath)
                summary = self.generate_summary(text, filename)
                self.summaries[filename] = summary
                
                summary_path = os.path.join(self.summary_dir, f"{filename}.summary")
                with open(summary_path, 'w', encoding='utf-8') as f:
                    f.write(summary)
                print(f"âœ… æ‘˜è¦: {filename}")
            
            # æ­¥éª¤2: æ¸…æ´—åŸå§‹æ–‡æ¡£
            print("=== æ­¥éª¤2: æ¸…æ´—åŸå§‹æ–‡æ¡£ ===")
            for filename in files:
                filepath = os.path.join(input_dir, filename)
                text = self.read_file(filepath)
                cleaned = self.clean_text_chunks(text)
                self.cleaned_originals[filename] = cleaned
                
                original_path = os.path.join(self.cleaned_original_dir, f"{filename}.cleaned_original")
                with open(original_path, 'w', encoding='utf-8') as f:
                    f.write(cleaned)
                print(f"âœ… æ¸…æ´—åŸå§‹: {filename}")
            
            # æ­¥éª¤3: æ¸…æ´—æ‘˜è¦
            print("=== æ­¥éª¤3: æ¸…æ´—æ‘˜è¦ ===")
            for filename in files:
                summary = self.summaries[filename]
                cleaned_summary = self.clean_text_chunks(summary)
                self.cleaned_summaries[filename] = cleaned_summary
                
                summary_path = os.path.join(self.cleaned_summary_dir, f"{filename}.cleaned_summary")
                with open(summary_path, 'w', encoding='utf-8') as f:
                    f.write(cleaned_summary)
                print(f"âœ… æ¸…æ´—æ‘˜è¦: {filename}")
        
        # æ­¥éª¤4: é…å¯¹ä¸Šä¼ åˆ°çŸ¥è¯†åº“
        if self.cleaned_summaries or self.cleaned_originals:
            print("=== æ­¥éª¤4: é…å¯¹ä¸Šä¼ åˆ°DifyçŸ¥è¯†åº“ ===")
            
            upload_success = self.upload_paired_documents()
            
            if upload_success:
                # æ¸…ç†æ‰€æœ‰ä¸­é—´æ–‡ä»¶
                for temp_dir in [self.summary_dir, self.cleaned_original_dir, self.cleaned_summary_dir]:
                    if os.path.exists(temp_dir):
                        for filename in os.listdir(temp_dir):
                            os.remove(os.path.join(temp_dir, filename))
                print("âœ… æ‰€æœ‰æ–‡æ¡£é…å¯¹ä¸Šä¼ æˆåŠŸï¼Œå·²æ¸…ç†ä¸­é—´æ–‡ä»¶ï¼")
            else:
                print("âš ï¸ éƒ¨åˆ†æ–‡æ¡£ä¸Šä¼ å¤±è´¥ï¼Œä¿ç•™ä¸­é—´æ–‡ä»¶ä»¥ä¾¿é‡æ–°å¤„ç†")
        else:
            print("âœ… æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    handler = DocumentHandler()
    handler.process_documents("input_files")