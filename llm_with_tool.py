import os
import asyncio
import json
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from dotenv import load_dotenv
from dual_stage_retrieval_tool import dual_stage_retrieve

load_dotenv()

async def stream_agent_response(agent, input_data):
    """ç»Ÿä¸€çš„æµå¼è¾“å‡ºå¤„ç†"""
    content=""
    structured_response=None
    async for event in agent.astream_events(input_data):
        if event.get("event") == "on_chain_end":
            event_data = event.get("data", {})
            if "output" in event_data and "structured_response" in event_data["output"]:
                structured_response = event_data["output"]["structured_response"]
        else:
            chunk = event.get("data", {}).get("chunk", {})
            if hasattr(chunk, 'content') and chunk.content:
                print(chunk.content, end="", flush=True)
                content+=chunk.content

    return structured_response,content

async def stream_with_token_output():
    llm = ChatOpenAI(
        model="google/gemini-2.5-flash",
        api_key=os.getenv('OPENROUTER_API_KEY'),
        base_url=os.getenv('OPENROUTER_BASE_URL', 'https://openrouter.ai/api/v1'),
        temperature=0.7,
        streaming=True
    )
    
    agent = create_react_agent(
        model=llm,
        tools=[dual_stage_retrieve],
        prompt="""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¹¡æ‘æŒ¯å…´é¡¾é—®AIåŠ©æ‰‹ï¼Œæ‹¥æœ‰è®¿é—®æŒ¯å…´ä¹¡æ‘æ¡ˆä¾‹çŸ¥è¯†åº“çš„èƒ½åŠ›ã€‚
        
        ## çŸ¥è¯†åº“æ£€ç´¢å·¥å…·ä½¿ç”¨æŒ‡å—ï¼š
        
        ### æ ¸å¿ƒåŸåˆ™ï¼š
        1. **æç‚¼å…³é”®è¯**ï¼šä¸è¦å°†ç”¨æˆ·çš„å®Œæ•´é—®é¢˜ç›´æ¥ä¼ å…¥å·¥å…·ï¼Œè€Œæ˜¯æç‚¼å‡ºæ ¸å¿ƒå…³é”®è¯è¿›è¡Œæ£€ç´¢
        2. **å¤šæ¬¡æ£€ç´¢**ï¼šå¯ä»¥å¤šæ¬¡è°ƒç”¨å·¥å…·è·å–æ›´å®Œæ•´çš„ä¿¡æ¯ï¼Œæ¯æ¬¡èšç„¦ä¸åŒå…³é”®è¯
        3. **æ¸è¿›å¼æŸ¥è¯¢**ï¼šå…ˆç”¨å®½æ³›å…³é”®è¯ï¼Œå†æ ¹æ®ç»“æœè¿›è¡Œç²¾å‡†æŸ¥è¯¢
        
        ### æ£€ç´¢ç­–ç•¥ï¼š
        - å°†å¤æ‚é—®é¢˜æ‹†è§£ä¸ºå¤šä¸ªæ ¸å¿ƒæ¦‚å¿µåˆ†åˆ«æ£€ç´¢
        - ä½¿ç”¨ç®€æ´çš„å…³é”®è¯è€Œéå®Œæ•´å¥å­
        - æ ¹æ®æ£€ç´¢ç»“æœè°ƒæ•´åç»­æŸ¥è¯¢ç­–ç•¥
        
        ### ä½¿ç”¨ç¤ºä¾‹ï¼š
        - ç”¨æˆ·é—®"å¦‚ä½•åœ¨å±±åŒºå‘å±•ç”Ÿæ€å†œä¸šäº§ä¸šåŒ–ç»è¥æ¨¡å¼"ï¼Œåº”è¯¥ï¼š
          1. å…ˆæ£€ç´¢"ç”Ÿæ€å†œä¸š"è·å–åŸºç¡€ä¿¡æ¯
          2. å†æ£€ç´¢"äº§ä¸šåŒ–ç»è¥"äº†è§£ç»è¥æ¨¡å¼
          3. æœ€åæ£€ç´¢"å±±åŒºå†œä¸š"è·å–åœ°åŸŸç‰¹è‰²æ¡ˆä¾‹
        
        ### æ³¨æ„äº‹é¡¹ï¼š
        - æ¯æ¬¡æ£€ç´¢åªä¼ å…¥3-5ä¸ªæ ¸å¿ƒå…³é”®è¯
        - é¿å…ä¼ å…¥å®Œæ•´çš„ç”¨æˆ·é—®é¢˜
        - å¤šè§’åº¦æ£€ç´¢æ¯”å•æ¬¡æ£€ç´¢æ›´èƒ½è·å¾—å…¨é¢ä¿¡æ¯
        
        è®°ä½ï¼šç”¨å…³é”®è¯æ£€ç´¢ï¼Œå¤šæ¬¡è°ƒç”¨ï¼Œç»¼åˆåˆ†æï¼
        """
    )
    
    test_query = """"
    æµ™æ±Ÿæ¹–å·æŸä¹¡æ‘æ‹¥æœ‰ä¸€ç‰‡é—²ç½®çš„å±±è°·æ—åœ°ï¼Œå‘¨è¾¹æœ‰æºªæµã€å°å‹ç€‘å¸ƒç­‰è‡ªç„¶æ™¯è§‚ï¼Œä½†é•¿æœŸå› ç¼ºä¹å¼€å‘è€Œå¤„äº
è’åºŸçŠ¶æ€ï¼Œå½“åœ°ä¹Ÿå¸Œæœ›é€šè¿‡å‘å±•æ–‡æ—…äº§ä¸šå¸¦åŠ¨ä¹¡æ‘æŒ¯å…´ï¼Œå´é¢ä¸´èµ„é‡‘æœ‰é™ã€ä¸šæ€è§„åˆ’ä¸æ¸…æ™°ã€æ‹…å¿ƒåŒè´¨åŒ–ç«äº‰
ç­‰é—®é¢˜ã€‚è‹¥è¯¥æ‘åº„è®¡åˆ’å€Ÿé‰´ç±»ä¼¼æ¨¡å¼æ‰“é€ éœ²è¥æ–‡æ—…é¡¹ç›®ï¼Œå¯ä»å“ªäº›æ–¹é¢å…¥æ‰‹ç ´è§£å‘å±•éš¾é¢˜ï¼Ÿå…¶æ€è·¯ä¸æµ™æ±Ÿå®‰å‰
åŠå²›ç†æƒ³æ‘ï¼ˆåŠå²›éœ²è¥æ‘2å·è¥åœ°ï¼‰çš„å®è·µæœ‰å“ªäº›å…±é€šä¹‹å¤„ï¼Ÿ
    """
    print(f"ğŸ¤– æŸ¥è¯¢: {test_query}")
    print("=" * 60)
    print()
    structured_response,content=await stream_agent_response(agent, {"messages": [{"role": "user", "content": test_query}]})

    

if __name__ == "__main__":
    asyncio.run(stream_with_token_output())