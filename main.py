import os
import time
from pathlib import Path
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed, Future
from threading import Lock
from dataclasses import dataclass
from tqdm import tqdm
import threading

from config import Config
from document_processor import DocumentProcessor
from text_splitter import TextSplitter
from llm_client import LLMClient

@dataclass
class ChunkTask:
    """æ–‡æœ¬ç‰‡æ®µå¤„ç†ä»»åŠ¡"""
    file_path: str
    chunk_index: int
    chunk_text: str
    is_first_chunk: bool

@dataclass
class ChunkResult:
    """æ–‡æœ¬ç‰‡æ®µå¤„ç†ç»“æœ"""
    file_path: str
    chunk_index: int
    formatted_chunk: str
    success: bool
    error: str = None

class GlobalResourcePool:
    """å…¨å±€èµ„æºæ± ï¼Œç®¡ç†å¹¶å‘å¤„ç†"""
    
    def __init__(self, max_workers: int = 10):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.llm_client = LLMClient()
        self.lock = Lock()
        self.active_tasks = {}  # file_path -> set of futures
        self.progress_bars = {}  # file_path -> tqdm progress bar
        
    def submit_chunk_task(self, task: ChunkTask) -> Future:
        """æäº¤æ–‡æœ¬ç‰‡æ®µå¤„ç†ä»»åŠ¡"""
        future = self.executor.submit(self._process_chunk, task)
        
        with self.lock:
            if task.file_path not in self.active_tasks:
                self.active_tasks[task.file_path] = set()
            self.active_tasks[task.file_path].add(future)
        
        return future
    
    def _process_chunk(self, task: ChunkTask) -> ChunkResult:
        """å¤„ç†å•ä¸ªæ–‡æœ¬ç‰‡æ®µ"""
        try:
            # è°ƒç”¨LLMæ¸…æ´—æ–‡æœ¬ï¼Œè¿”å›æ ¼å¼ï¼šåŸæ–‡ç‰‡æ®µ\n###å…³é”®è¯1 ###å…³é”®è¯2
            cleaned_result = self.llm_client.clean_text(task.chunk_text)
            
            # æ ¹æ®æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ªç‰‡æ®µæ¥å†³å®šæ ¼å¼
            if task.is_first_chunk:
                # ç¬¬ä¸€ä¸ªç‰‡æ®µä¸åŠ &&&&å‰ç¼€
                formatted_chunk = cleaned_result
            else:
                # åç»­ç‰‡æ®µåŠ &&&&å‰ç¼€
                formatted_chunk = f"&&&&\n{cleaned_result}"
            
            # æ›´æ–°è¿›åº¦æ¡
            self._update_progress(task.file_path)
            
            return ChunkResult(
                file_path=task.file_path,
                chunk_index=task.chunk_index,
                formatted_chunk=formatted_chunk,
                success=True
            )
            
        except Exception as e:
            # å¤„ç†å¤±è´¥æ—¶ä½¿ç”¨åŸæ–‡æœ¬
            if task.is_first_chunk:
                formatted_chunk = f"{task.chunk_text}\n###å¤„ç†å¤±è´¥"
            else:
                formatted_chunk = f"&&&&\n{task.chunk_text}\n###å¤„ç†å¤±è´¥"
            
            # æ›´æ–°è¿›åº¦æ¡
            self._update_progress(task.file_path)
            
            return ChunkResult(
                file_path=task.file_path,
                chunk_index=task.chunk_index,
                formatted_chunk=formatted_chunk,
                success=False,
                error=str(e)
            )
    
    def _update_progress(self, file_path: str):
        """æ›´æ–°è¿›åº¦æ¡"""
        with self.lock:
            if file_path in self.progress_bars:
                self.progress_bars[file_path].update(1)
    
    def create_progress_bar(self, file_path: str, total_chunks: int):
        """ä¸ºæ–‡ä»¶åˆ›å»ºè¿›åº¦æ¡"""
        file_name = Path(file_path).name
        with self.lock:
            self.progress_bars[file_path] = tqdm(
                total=total_chunks,
                desc=f"ğŸ”„ {file_name}",
                unit="ç‰‡æ®µ",
                ncols=100,
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
            )
    
    def wait_for_file_completion(self, file_path: str, total_chunks: int) -> List[ChunkResult]:
        """ç­‰å¾…æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰ä»»åŠ¡å®Œæˆ"""
        results = []
        file_name = Path(file_path).name
        
        with self.lock:
            futures = self.active_tasks.get(file_path, set()).copy()
        
        if not futures:
            return results
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in as_completed(futures):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(f"\nâš ï¸  [{file_name}] ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
        
        # å…³é—­è¿›åº¦æ¡
        with self.lock:
            if file_path in self.progress_bars:
                self.progress_bars[file_path].close()
                del self.progress_bars[file_path]
            
            # æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡
            if file_path in self.active_tasks:
                del self.active_tasks[file_path]
        
        print(f"âœ… [{file_name}] å¤„ç†å®Œæˆ!")
        return results
    
    def get_active_task_count(self) -> int:
        """è·å–å½“å‰æ´»è·ƒä»»åŠ¡æ•°é‡"""
        with self.lock:
            return sum(len(futures) for futures in self.active_tasks.values())
    
    def get_active_files(self) -> List[str]:
        """è·å–å½“å‰æ­£åœ¨å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨"""
        with self.lock:
            return [Path(file_path).name for file_path in self.active_tasks.keys()]
    
    def shutdown(self):
        """å…³é—­èµ„æºæ± """
        # å…³é—­æ‰€æœ‰è¿›åº¦æ¡
        with self.lock:
            for pbar in self.progress_bars.values():
                pbar.close()
            self.progress_bars.clear()
        
        self.executor.shutdown(wait=True)

class DataCleaningPipeline:
    """æ•°æ®æ¸…æ´—æµæ°´çº¿"""
    
    def __init__(self, resource_pool: GlobalResourcePool):
        self.config = Config()
        self.doc_processor = DocumentProcessor()
        self.text_splitter = TextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            overlap_size=Config.OVERLAP_SIZE
        )
        self.resource_pool = resource_pool
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Config.ensure_directories()
    
    def process_file(self, file_path: str) -> Dict:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        file_name = Path(file_path).name
        print(f"\n{'='*60}")
        print(f"ğŸ“ å¼€å§‹å¤„ç†æ–‡ä»¶: {file_name}")
        print(f"{'='*60}")
        
        try:
            # 1. è¯»å–æ–‡ä»¶å†…å®¹
            print(f"ğŸ“– è¯»å–æ–‡ä»¶å†…å®¹...")
            text = self.doc_processor.read_file(file_path)
            print(f"âœ… æ–‡ä»¶è¯»å–å®Œæˆï¼Œé•¿åº¦: {len(text):,} å­—ç¬¦")
            
            # 2. åˆ†å‰²æ–‡æœ¬
            print(f"âœ‚ï¸  åˆ†å‰²æ–‡æœ¬...")
            chunks = self.text_splitter.split_text(text)
            print(f"âœ… æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} ä¸ªç‰‡æ®µ")
            
            # 3. åˆ›å»ºè¿›åº¦æ¡
            self.resource_pool.create_progress_bar(file_path, len(chunks))
            
            # 4. æäº¤æ‰€æœ‰ç‰‡æ®µåˆ°å…¨å±€èµ„æºæ± 
            print(f"ğŸš€ æäº¤ç‰‡æ®µåˆ°å¤„ç†é˜Ÿåˆ—...")
            self._submit_chunks_to_pool(file_path, chunks)
            
            # 5. ç­‰å¾…æ‰€æœ‰ç‰‡æ®µå¤„ç†å®Œæˆ
            results = self.resource_pool.wait_for_file_completion(file_path, len(chunks))
            
            # 6. æŒ‰é¡ºåºæ•´ç†ç»“æœ
            print(f"ğŸ“‹ æ•´ç†å¤„ç†ç»“æœ...")
            processed_chunks = self._organize_results(results, len(chunks))
            
            # 7. è¿æ¥æ‰€æœ‰ç‰‡æ®µ - ä½¿ç”¨æ¢è¡Œè¿æ¥
            final_content = '\n'.join(processed_chunks)
            
            # 8. ä¿å­˜ç»“æœ
            print(f"ğŸ’¾ ä¿å­˜å¤„ç†ç»“æœ...")
            output_path = self._save_result(file_path, final_content, len(chunks))
            
            result = {
                'status': 'success',
                'input_file': file_path,
                'output_file': output_path,
                'original_length': len(text),
                'final_length': len(final_content),
                'chunks_count': len(chunks)
            }
            
            print(f"ğŸ‰ æ–‡ä»¶å¤„ç†å®Œæˆ: {Path(output_path).name}")
            print(f"{'='*60}")
            return result
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
            print(f"{'='*60}")
            return {
                'status': 'error',
                'input_file': file_path,
                'error': str(e)
            }
    
    def _submit_chunks_to_pool(self, file_path: str, chunks: List[str]):
        """å°†æ‰€æœ‰æ–‡æœ¬ç‰‡æ®µæäº¤åˆ°å…¨å±€èµ„æºæ± """
        for i, chunk in enumerate(chunks):
            task = ChunkTask(
                file_path=file_path,
                chunk_index=i,
                chunk_text=chunk,
                is_first_chunk=(i == 0)
            )
            self.resource_pool.submit_chunk_task(task)
        
        active_count = self.resource_pool.get_active_task_count()
        print(f"âœ… å·²æäº¤ {len(chunks)} ä¸ªç‰‡æ®µï¼Œå½“å‰é˜Ÿåˆ—: {active_count} ä¸ªä»»åŠ¡")
    
    def _organize_results(self, results: List[ChunkResult], total_chunks: int) -> List[str]:
        """æŒ‰é¡ºåºæ•´ç†å¤„ç†ç»“æœ"""
        # æŒ‰chunk_indexæ’åº
        results.sort(key=lambda x: x.chunk_index)
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç‰‡æ®µéƒ½å¤„ç†å®Œæˆ
        if len(results) != total_chunks:
            raise Exception(f"ç‰‡æ®µå¤„ç†ä¸å®Œæ•´: æœŸæœ›{total_chunks}ä¸ªï¼Œå®é™…{len(results)}ä¸ª")
        
        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„ç‰‡æ®µ
        success_count = sum(1 for r in results if r.success)
        failure_count = total_chunks - success_count
        
        print(f"ğŸ“ˆ å¤„ç†ç»Ÿè®¡: âœ…æˆåŠŸ {success_count} ä¸ªï¼ŒâŒå¤±è´¥ {failure_count} ä¸ª")
        
        if failure_count > 0:
            print("âš ï¸  å¤±è´¥çš„ç‰‡æ®µ:")
            for result in results:
                if not result.success:
                    print(f"   âŒ ç‰‡æ®µ {result.chunk_index + 1}: {result.error}")
        
        return [result.formatted_chunk for result in results]
    
    def _save_result(self, input_file: str, content: str, chunks_count: int) -> str:
        """ä¿å­˜æ¸…æ´—ç»“æœ"""
        input_path = Path(input_file)
        output_filename = f"{input_path.stem}_cleaned.md"
        output_path = Path(Config.OUTPUT_DIR) / output_filename
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        return str(output_path)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ•°æ®æ¸…æ´—å·¥å…·å¯åŠ¨")
    print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {Config.INPUT_DIR}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {Config.OUTPUT_DIR}")
    print(f"ğŸ”§ å…¨å±€èµ„æºæ± å¤§å°: 10 ä¸ªå¹¶å‘ä»»åŠ¡")
    print(f"ğŸ“‹ æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(Config.SUPPORTED_EXTENSIONS)}")
    
    # åˆ›å»ºå…¨å±€èµ„æºæ± 
    resource_pool = GlobalResourcePool(max_workers=10)
    
    try:
        # è·å–æ‰€æœ‰å¾…å¤„ç†æ–‡ä»¶
        files_to_process = []
        input_dir = Path(Config.INPUT_DIR)
        for file_path in input_dir.iterdir():
            if file_path.suffix.lower() in Config.SUPPORTED_EXTENSIONS:
                files_to_process.append(str(file_path))
        
        if not files_to_process:
            print("âŒ æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
            return
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(files_to_process)} ä¸ªæ–‡ä»¶å¾…å¤„ç†:")
        for i, file_path in enumerate(files_to_process, 1):
            file_size = Path(file_path).stat().st_size
            print(f"   {i}. {Path(file_path).name} ({file_size:,} å­—èŠ‚)")
        
        # åˆ›å»ºæ•°æ®æ¸…æ´—æµæ°´çº¿
        pipeline = DataCleaningPipeline(resource_pool)
        
        # å¤„ç†æ‰€æœ‰æ–‡ä»¶
        results = []
        for i, file_path in enumerate(files_to_process, 1):
            print(f"\nğŸ”„ å¤„ç†è¿›åº¦: {i}/{len(files_to_process)}")
            result = pipeline.process_file(file_path)
            results.append(result)
        
        # è¾“å‡ºæ€»ç»“
        print(f"\n{'='*60}")
        print("ğŸ“Š å¤„ç†æ€»ç»“")
        print(f"{'='*60}")
        
        success_count = sum(1 for r in results if r['status'] == 'success')
        total_files = len(results)
        
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files}")
        print(f"âœ… æˆåŠŸå¤„ç†: {success_count}")
        print(f"âŒ å¤„ç†å¤±è´¥: {total_files - success_count}")
        
        if success_count > 0:
            print(f"\nğŸ“‹ æˆåŠŸå¤„ç†çš„æ–‡ä»¶:")
            for result in results:
                if result['status'] == 'success':
                    print(f"   âœ… {Path(result['input_file']).name} -> {Path(result['output_file']).name}")
                    print(f"      ğŸ“Š {result['original_length']:,} -> {result['final_length']:,} å­—ç¬¦")
                    print(f"      ğŸ§© {result['chunks_count']} ä¸ªç‰‡æ®µ")
        
        if total_files - success_count > 0:
            print(f"\nâŒ å¤„ç†å¤±è´¥çš„æ–‡ä»¶:")
            for result in results:
                if result['status'] == 'error':
                    print(f"   âŒ {Path(result['input_file']).name}: {result['error']}")
        
        print(f"\nğŸ‰ æ•°æ®æ¸…æ´—å®Œæˆ!")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
    finally:
        # å…³é—­èµ„æºæ± 
        resource_pool.shutdown()
        print("ğŸ”š èµ„æºæ± å·²å…³é—­")

if __name__ == "__main__":
    main()